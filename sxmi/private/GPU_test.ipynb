{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from itertools import compress\n",
    "\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "import h5py\n",
    "\n",
    "from dscribe.descriptors import SOAP\n",
    "from dscribe.kernels import AverageKernel\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_soap_descriptors(structures, njobs, species, r_cut, n_max, l_max, logger):\n",
    "    \"\"\"\n",
    "    Function: Compute SOAP descriptors for a list of structures\n",
    "    Input:\n",
    "        SOAP inputs\n",
    "        logger: logger object\n",
    "    Output:\n",
    "        List of SOAP descriptors in numpy.ndarray format\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    soap = SOAP(\n",
    "        species=species,\n",
    "        r_cut=r_cut,\n",
    "        n_max=n_max,\n",
    "        l_max=l_max\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        soap_descriptors = soap.create(structures, n_jobs=njobs)\n",
    "    except IndexError:\n",
    "        # 说明 structures 为空\n",
    "        soap_descriptors = []\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"SOAP descriptors computed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # 由于此处调用本函数计算时，structures 中的结构默认是化学式相同的，因此返回的是 np.ndarray\n",
    "    # 需要返回 list 类型，其中元素为 np.ndarray\n",
    "    if type(soap_descriptors) == np.ndarray:\n",
    "        return [i for i in soap_descriptors]\n",
    "    \n",
    "    # 原子数不相同时直接返回 list 即可\n",
    "    return soap_descriptors\n",
    "\n",
    "def compute_similarity(cand_soap, ref_soap, kernel_metric=\"laplacian\"):\n",
    "    \"\"\"\n",
    "    Function: Compute the similarity between candidate and reference SOAP descriptors using the specified kernel metric.\n",
    "    Input:\n",
    "        cand_soap: Candidate SOAP descriptor\n",
    "        ref_soap: Reference SOAP descriptor\n",
    "        kernel_metric: Kernel metric to use for similarity computation (default: laplacian)\n",
    "    Output:\n",
    "        Similarity score between candidate and reference SOAP descriptors\n",
    "    \"\"\"\n",
    "    re = AverageKernel(metric=kernel_metric)\n",
    "    return re.create(cand_soap, ref_soap)\n",
    "\n",
    "def compare_and_update_structures(ref_structures, cand_structures, njobs=8, species=[\"H\", \"C\", \"O\", \"N\"], r_cut=10.0, n_max=6, l_max=4, threshold=0.9, logger=None):\n",
    "    \"\"\"\n",
    "    Function:\n",
    "    Compare candidate structures with reference structures.\n",
    "    Update the reference database one molecule by one molecule.\n",
    "\n",
    "    Input:\n",
    "        ref_structures: list of ase.Atoms objects, reference structures\n",
    "        cand_structures: list of ase.Atoms objects, candidate structures\n",
    "        njobs: int, number of jobs to run in parallel\n",
    "        species: list of str, species to consider  \n",
    "        r_cut: float, cutoff radius for SOAP calculation\n",
    "        n_max: int, number of radial basis functions\n",
    "        l_max: int, maximum degree of spherical harmonics\n",
    "        threshold: float, similarity threshold for reducing candidate structures\n",
    "        logger: logging.Logger object, logger for logging\n",
    "\n",
    "    Output:\n",
    "        ref_structures: list of ase.Atoms objects, updated reference structures\n",
    "        soap_ref: list of soap_descriptors, SOAP descriptors for updated reference structures\n",
    "    \"\"\"\n",
    "    round_num = 0\n",
    "    logger.info(f\"njobs: {njobs}, species: {species}, r_cut: {r_cut}, n_max: {n_max}, l_max: {l_max}, threshold: {threshold}\")\n",
    "\n",
    "    while True:\n",
    "        round_num += 1\n",
    "\n",
    "        if round_num == 1:\n",
    "            # 初次计算全部的 SOAP 描述符\n",
    "            # 这里还可以改进，先计算描述符或直接读入描述符\n",
    "            soap_ref = compute_soap_descriptors(ref_structures, njobs, species, r_cut, n_max, l_max, logger)\n",
    "            soap_cand = compute_soap_descriptors(cand_structures, njobs, species, r_cut, n_max, l_max, logger)\n",
    "\n",
    "            # 如果 soap_ref 为空，则将 soap_cand 的第一个元素添加到 soap_ref 中\n",
    "            if soap_ref == []:\n",
    "                ref_structures.append(cand_structures[0])\n",
    "                soap_ref.append(soap_cand[0])\n",
    "                logger.info(\"Ref structure is empty, add the first Cand structure to Ref structure\")\n",
    "\n",
    "            # 并行计算 cand_structures 中每个结构与 ref_structures 中所有结构的相似度\n",
    "            start_time = time.time()\n",
    "            re_kernel_results = Parallel(n_jobs=njobs)(delayed(compute_similarity)(soap_cand[i:i+1], soap_ref) for i in range(len(soap_cand)))\n",
    "            re_kernel = np.vstack(re_kernel_results)\n",
    "            end_time = time.time()\n",
    "            logger.info(f\"Round {round_num}: Similarity computation completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            print('Round:', round_num)\n",
    "            print('re_kernel shape:', re_kernel.shape)\n",
    "            print('re_kernel:', re_kernel[:10])\n",
    "\n",
    "            # 选取 cand_structures 中每个结构与 ref_structures 中所有结构的最大相似度\n",
    "            max_similarity_values = np.max(re_kernel, axis=1)\n",
    "        \n",
    "        else:\n",
    "            # 并行计算 cand_structures 中每个结构与 ref_structures 新加入的结构的相似度\n",
    "            # 变量覆盖释放内存空间\n",
    "            ### 这里可以写成一个函数，便于复用\n",
    "            start_time = time.time()\n",
    "            re_kernel_results = Parallel(n_jobs=njobs)(delayed(compute_similarity)(soap_cand[i:i+1], [soap_ref[-1]]) for i in range(len(soap_cand)))\n",
    "            re_kernel = np.vstack(re_kernel_results)\n",
    "            end_time = time.time()\n",
    "            logger.info(f\"Round {round_num}: Similarity computation completed in {end_time - start_time:.2f} seconds\")\n",
    "            ### 这里可以写成一个函数，便于复用\n",
    "\n",
    "            print('Round:', round_num)\n",
    "            print('re_kernel shape:', re_kernel.shape)\n",
    "            print('re_kernel:', re_kernel[:10])\n",
    "\n",
    "            # 将原先 max_similarity_values 与新加入的点的堆叠\n",
    "            max_similarity_values = np.max(np.column_stack((max_similarity_values, np.max(re_kernel, axis=1))), axis=1)\n",
    "\n",
    "\n",
    "        # 删除 cand_structures 中与 ref_structures 中相似度高于 threshold 的所有结构\n",
    "        # 更新 soap_cand, cand_structures, max_similarity_values\n",
    "        old_cand_num = len(cand_structures)\n",
    "\n",
    "        preserve_condition = max_similarity_values < threshold # 减少不必要的 round(5) 开销\n",
    "        soap_cand = list(compress(soap_cand, preserve_condition)) # itertools.compress() 更高效\n",
    "        cand_structures = list(compress(cand_structures, preserve_condition))\n",
    "        max_similarity_values = max_similarity_values[preserve_condition] # np 布尔索引更高效\n",
    "        \n",
    "        new_cand_num = len(cand_structures)\n",
    "        logger.info(f\"Round {round_num}: Cand structures reduced from {old_cand_num} to {new_cand_num}\")\n",
    "\n",
    "        # 如果 cand_structures 中没有元素，则退出循环\n",
    "        if new_cand_num == 0:\n",
    "            break\n",
    "\n",
    "        # 将 cand_structures 与 ref_structures 中最不相似的结构添加到 ref_structures 中\n",
    "        min_max_similarity = np.min(max_similarity_values).round(5) # 减少不必要的 round(5) 开销\n",
    "        min_max_similarity_index = np.argmin(max_similarity_values)\n",
    "        ref_structures.append(cand_structures[min_max_similarity_index])\n",
    "        soap_ref.append(soap_cand[min_max_similarity_index])\n",
    "        logger.info(f\"Round {round_num}: Added structure with min max similarity {min_max_similarity}.\")\n",
    "        logger.info(f\"Ref structures: {len(ref_structures)}, Cand structures: {len(cand_structures)}\")\n",
    "        logger.info(\"---------\")\n",
    "\n",
    "\n",
    "    logger.info(\"No structures remaining in candidate list.\")\n",
    "    logger.info(f\"Ref structures: {len(ref_structures)}, Cand structures: {len(cand_structures)}\")\n",
    "    logger.info(\"---------\")\n",
    "        \n",
    "    return ref_structures, soap_ref\n",
    "\n",
    "def save_soap_to_hdf5(soap_dict, hdf5_name):\n",
    "    \"\"\"\n",
    "    Save SOAP descriptors to an HDF5 file.\n",
    "\n",
    "    Example input structure:\n",
    "    defaultdict(list, {\n",
    "        'C2H3N3O': [\n",
    "            array([[...], [...], ...]),  # SOAP descriptors for the first molecule\n",
    "            array([[...], [...], ...])   # SOAP descriptors for the second molecule\n",
    "        ],\n",
    "        'C2H6': [\n",
    "            array([[...], [...], ...])   # SOAP descriptors for another molecule\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    - Each value is an array representing the SOAP descriptors for a molecule, with shape (N, M),\n",
    "    where N is the number of descriptors and M is the dimension of each descriptor.\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_name, \"w\") as hdf:\n",
    "        for formula, soap_list in soap_dict.items():\n",
    "            stacked_soap = np.array(soap_list)\n",
    "            hdf.create_dataset(formula, data=stacked_soap)\n",
    "\n",
    "# 在 .py 中暂时未使用\n",
    "def read_soap_from_hdf5(hdf5_name):\n",
    "    \"\"\"\n",
    "    Read SOAP descriptors from an HDF5 file.\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(list): A dictionary with molecule formulas as keys\n",
    "        and lists of corresponding SOAP descriptors as values.\n",
    "    \"\"\"\n",
    "    soap_dict = defaultdict(list)\n",
    "\n",
    "    with h5py.File(hdf5_name, \"r\") as hdf:\n",
    "        for formula in hdf.keys():\n",
    "            soap_descriptors = hdf[formula][:]\n",
    "            soap_dict[formula].append(soap_descriptors)\n",
    "\n",
    "    return soap_dict\n",
    "\n",
    "# 在 .py 中暂时未使用\n",
    "def defaultdict_profiler(soap_data):\n",
    "    \"\"\"\n",
    "    Print the available formulas and the shape of their corresponding SOAP descriptors.\n",
    "    \"\"\"\n",
    "    formulas = list(soap_data.keys())\n",
    "    print(\"Available formulas:\", formulas)\n",
    "\n",
    "    # 遍历每个分子式并读取 SOAP 描述符\n",
    "    for formula in formulas:\n",
    "        soap_descriptors = soap_data[formula][:][0]\n",
    "        print(f\"Formula: {formula}, Shape of SOAP descriptors: {soap_descriptors.shape}\")\n",
    "    print(\"---------\")\n",
    "\n",
    "# 设置总的日志记录\n",
    "def setup_total_logging():\n",
    "    '''\n",
    "    Setup the total logging for the algorithm.\n",
    "    '''\n",
    "    # 创建总的 Logger\n",
    "    total_logger = logging.getLogger(\"total_logger\")\n",
    "    total_logger.setLevel(logging.INFO)\n",
    "\n",
    "    # 清除之前的处理器，确保每次都干净\n",
    "    for handler in total_logger.handlers[:]:\n",
    "        total_logger.removeHandler(handler)\n",
    "\n",
    "    # 创建文件处理器\n",
    "    file_handler = logging.FileHandler(\"total_output.log\", mode='w')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    total_logger.addHandler(file_handler)\n",
    "\n",
    "    return total_logger\n",
    "\n",
    "# 设置各化学式的日志记录\n",
    "def setup_logging(reaction_formula):\n",
    "    '''\n",
    "    Setup the logging for each chemical formula.\n",
    "    '''\n",
    "    # 创建新的 Logger\n",
    "    logger = logging.getLogger(reaction_formula)\n",
    "    logger.setLevel(logging.INFO)\n",
    "        \n",
    "    # 清除之前的处理器，确保每次都干净\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    # 创建文件夹\n",
    "    os.makedirs(reaction_formula, exist_ok=True)\n",
    "\n",
    "    # 创建文件处理器\n",
    "    file_handler = logging.FileHandler(os.path.join(reaction_formula, f\"{reaction_formula}_output.log\"), mode='w')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    \n",
    "    # 添加处理器到 Logger\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# 主程序\n",
    "def main(ref_file, cand_file, njobs, r_cut, n_max, l_max, threshold):\n",
    "    total_logger = setup_total_logging()\n",
    "    total_logger.info('Total Log begin')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 读取数据\n",
    "    if ref_file == '':\n",
    "        ref_structures = []\n",
    "    else:\n",
    "        ref_structures = read(ref_file, index=':')\n",
    "    cand_structures = read(cand_file, index=':')\n",
    "\n",
    "    # 根据 chemical_formula 分组\n",
    "    ref_dict = defaultdict(list)\n",
    "    cand_dict = defaultdict(list)\n",
    "\n",
    "    for structure in ref_structures:\n",
    "        formula = structure.get_chemical_formula()\n",
    "        ref_dict[formula].append(structure)\n",
    "\n",
    "    for structure in cand_structures:\n",
    "        formula = structure.get_chemical_formula()\n",
    "        cand_dict[formula].append(structure)\n",
    "\n",
    "    formula_num = len(cand_dict.keys())\n",
    "    total_logger.info(f\"There are {formula_num} formulas to process.\")\n",
    "    total_logger.info(\"---------\")\n",
    "\n",
    "\n",
    "    for i, formula in enumerate(cand_dict.keys()):\n",
    "        # 如果 ref_dict 中没有该组，则会返回空列表，程序可以正常运行\n",
    "        total_logger.info(f\"Processing formula {i+1:>}/{formula_num:>}: {formula}\")\n",
    "        total_logger.info(f\"Start Ref structures: {len(ref_dict[formula])}, Cand structures: {len(cand_dict[formula])}\")\n",
    "\n",
    "        logger = setup_logging(formula)  \n",
    "        formula_start_time = time.time()\n",
    "        logger.info('Log begin')\n",
    "        logger.info(f\"Processing formula: {formula}\")\n",
    "\n",
    "        # threshold 对于 species 很敏感，这可能是自动匹配 species 后可能出现的问题\n",
    "        # species=list(set(cand_dict[formula][0].get_chemical_symbols()))\n",
    "        updated_structures, updated_soap_list = compare_and_update_structures(ref_dict[formula], \n",
    "                                                                            cand_dict[formula], \n",
    "                                                                            njobs=njobs,\n",
    "                                                                            # species=species,\n",
    "                                                                            r_cut=r_cut,\n",
    "                                                                            n_max=n_max,\n",
    "                                                                            l_max=l_max,\n",
    "                                                                            threshold=threshold,\n",
    "                                                                            logger=logger)\n",
    "        \n",
    "        # 逐一保存更新后的参考结构\n",
    "        write(os.path.join(formula, f\"updated_ref_structures_{formula}.xyz\"), updated_structures)\n",
    "        logger.info(f\"Updated reference structures saved to '{formula}/updated_ref_structures_{formula}.xyz'\")\n",
    "        \n",
    "        # 保存更新后的结构和 SOAP 到 HDF5\n",
    "        soap_dict = defaultdict(list)\n",
    "        for i in range(len(updated_soap_list)):\n",
    "            soap_result = updated_soap_list[i]\n",
    "            soap_dict[formula].append(soap_result)\n",
    "        save_soap_to_hdf5(soap_dict, os.path.join(formula, f\"updated_ref_soap_descriptors_{formula}.h5\"))\n",
    "        logger.info(f\"SOAP descriptors saved to '{formula}/updated_ref_soap_descriptors_{formula}.h5'\")\n",
    "\n",
    "        formula_end_time = time.time()\n",
    "        logger.info(f\"Done! Total time elapsed: {formula_end_time - formula_start_time:.2f} seconds\")\n",
    "        logger.info('Log end')\n",
    "\n",
    "        total_logger.info(f\"End Ref structures: {len(updated_structures)}\")\n",
    "        total_logger.info(f\"Done! Total time elapsed: {formula_end_time - formula_start_time:.2f} seconds\")\n",
    "        total_logger.info(\"---------\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_logger.info('All reactions processed successfully!')\n",
    "    total_logger.info(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    total_logger.info('Total Log end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--ref REF] --cand CAND [--njobs NJOBS]\n",
      "                             [--r_cut R_CUT] [--n_max N_MAX] [--l_max L_MAX]\n",
      "                             [--threshold THRESHOLD]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --cand\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\dscribe\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Select different chemical structures.')\n",
    "    parser.add_argument('--ref', type=str, default='', help='Reference XYZ file')\n",
    "    parser.add_argument('--cand', type=str, required=True, help='Candidate XYZ file')\n",
    "    parser.add_argument('--njobs', type=int, default=8, help='Number of jobs for parallel processing')\n",
    "    parser.add_argument('--r_cut', type=float, default=10.0, help='Cutoff radius for soap descriptor')\n",
    "    parser.add_argument('--n_max', type=int, default=6, help='Number of radial basis functions')\n",
    "    parser.add_argument('--l_max', type=int, default=4, help='Maximum degree of spherical harmonics')\n",
    "    parser.add_argument('--threshold', type=float, default=0.9, help='Similarity threshold')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.ref, args.cand, args.njobs, args.r_cut, args.n_max, args.l_max, args.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1\n",
      "re_kernel shape: (100, 1)\n",
      "re_kernel: [[1.        ]\n",
      " [0.96039081]\n",
      " [0.92096803]\n",
      " [0.89363827]\n",
      " [0.85799494]\n",
      " [0.83940276]\n",
      " [0.82873544]\n",
      " [0.81831525]\n",
      " [0.81133494]\n",
      " [0.80643498]]\n",
      "Round: 2\n",
      "re_kernel shape: (86, 1)\n",
      "re_kernel: [[0.84577766]\n",
      " [0.87448632]\n",
      " [0.89647599]\n",
      " [0.93384489]\n",
      " [0.94597925]\n",
      " [0.96019293]\n",
      " [0.98173376]\n",
      " [1.        ]\n",
      " [0.84368853]\n",
      " [0.9000759 ]]\n",
      "Round: 3\n",
      "re_kernel shape: (73, 1)\n",
      "re_kernel: [[0.91029363]\n",
      " [0.93223138]\n",
      " [0.93010149]\n",
      " [0.91048445]\n",
      " [0.89514285]\n",
      " [0.96158465]\n",
      " [0.95521957]\n",
      " [0.97229951]\n",
      " [0.91670071]\n",
      " [0.9295723 ]]\n",
      "Round: 4\n",
      "re_kernel shape: (63, 1)\n",
      "re_kernel: [[0.96237323]\n",
      " [0.95877086]\n",
      " [0.9144786 ]\n",
      " [0.88941156]\n",
      " [0.87727563]\n",
      " [0.91379412]\n",
      " [0.95866378]\n",
      " [0.96058309]\n",
      " [1.        ]\n",
      " [0.92197856]]\n",
      "Round: 5\n",
      "re_kernel shape: (41, 1)\n",
      "re_kernel: [[0.94196679]\n",
      " [0.97565787]\n",
      " [0.9579    ]\n",
      " [0.84932731]\n",
      " [0.95744178]\n",
      " [0.91714586]\n",
      " [1.        ]\n",
      " [0.97408772]\n",
      " [0.9507878 ]\n",
      " [0.91297389]]\n",
      "Round: 6\n",
      "re_kernel shape: (23, 1)\n",
      "re_kernel: [[0.98272746]\n",
      " [0.88670294]\n",
      " [0.9666012 ]\n",
      " [0.96229922]\n",
      " [0.96904523]\n",
      " [0.96214242]\n",
      " [1.        ]\n",
      " [0.97332494]\n",
      " [0.89120928]\n",
      " [0.90210597]]\n",
      "Round: 7\n",
      "re_kernel shape: (6, 1)\n",
      "re_kernel: [[1.        ]\n",
      " [0.98263636]\n",
      " [0.83975318]\n",
      " [0.95230039]\n",
      " [0.98990381]\n",
      " [0.83782661]]\n",
      "Round: 8\n",
      "re_kernel shape: (2, 1)\n",
      "re_kernel: [[0.99371474]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "ref = ''\n",
    "cand = 'rxn0000.xyz'\n",
    "njobs = 2\n",
    "r_cut = 10.0\n",
    "n_max = 6\n",
    "l_max = 4\n",
    "threshold = 0.95\n",
    "main(ref, cand, njobs, r_cut, n_max, l_max, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试测试 GPU 代码，本机上没有 GPU，因此理论上结果应该与上述结果相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GPUAverageKernel:\n",
    "    def __init__(self, metric=\"laplacian\", gamma=None, degree=3, coef0=1, kernel_params=None, normalize_kernel=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metric (str): The pairwise metric used for calculating the local similarity, default is \"laplacian\".\n",
    "            gamma (float): Gamma parameter for Laplacian kernel. Default is None, use sklearn's default gamma.\n",
    "            degree (int): Degree of the polynomial kernel. Ignored for Laplacian. Default is 3.\n",
    "            coef0 (float): Zero coefficient for polynomial and sigmoid kernels. Ignored for Laplacian. Default is 1.\n",
    "            kernel_params (dict): Additional parameters for kernel function. Default is None.\n",
    "            normalize_kernel (bool): Whether to normalize the kernel. Default is True.\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.kernel_params = kernel_params\n",
    "        self.normalize_kernel = normalize_kernel\n",
    "\n",
    "    def get_pairwise_matrix(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Computes the pairwise similarity of atomic environments using Laplacian kernel on GPU.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Feature vector for atoms in structure A.\n",
    "            Y (np.ndarray): Feature vector for atoms in structure B.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: NxM matrix of local similarities between structures A and B.\n",
    "        \"\"\"\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        if Y is not None:\n",
    "            Y = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            Y = X\n",
    "        \n",
    "        print('X.shape:', X.shape)\n",
    "        print('Y.shape:', Y.shape)\n",
    "        \n",
    "        diff = torch.abs(X.unsqueeze(1) - Y.unsqueeze(0))\n",
    "        print(\"Difference between structures:\", diff.shape)\n",
    "\n",
    "        dist = torch.sum(diff, dim=2)\n",
    "        print(\"Distances:\", dist.shape)\n",
    "\n",
    "        K_ij = torch.exp(-self.gamma * dist)  # Laplacian kernel\n",
    "        print(\"Pairwise kernel:\", K_ij.shape)\n",
    "        \n",
    "        return K_ij\n",
    "\n",
    "    def get_global_similarity(self, localkernel):\n",
    "        \"\"\"\n",
    "        Computes the average global similarity between two structures.\n",
    "        \n",
    "        Args:\n",
    "            localkernel (np.ndarray): NxM matrix of local similarities between structures A and B.\n",
    "        \n",
    "        Returns:\n",
    "            float: Average similarity between the structures.\n",
    "        \"\"\"\n",
    "        # localkernel = localkernel.clone().detach().to(dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # print('localkernel:', localkernel)\n",
    "        K_ij = torch.mean(localkernel)\n",
    "        # print('Average similarity:', K_ij)\n",
    "        return K_ij.item()\n",
    "\n",
    "\n",
    "    def create(self, x, y=None):\n",
    "        \"\"\"\n",
    "        Creates the kernel matrix based on the given lists of local features x and y.\n",
    "        \n",
    "        Args:\n",
    "            x (iterable): A list of local feature arrays for each structure.\n",
    "            y (iterable): Optional second list of features. If not specified, it is assumed that y=x.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The pairwise global similarity kernel K[i,j] between the given structures.\n",
    "        \"\"\"\n",
    "        # Ensure y is provided or set y = x (symmetric case)\n",
    "        symmetric = False\n",
    "        if y is None:\n",
    "            y = x\n",
    "            symmetric = True\n",
    "\n",
    "        n_x = len(x)\n",
    "        n_y = len(y)\n",
    "        print('n_x', n_x)\n",
    "        print('n_y', n_y)\n",
    "\n",
    "        # Initialize the kernel matrix\n",
    "        K_ij = torch.zeros((n_x, n_y), dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Compute the kernel matrix (pairwise local similarity followed by global similarity)\n",
    "        for i in range(n_x):\n",
    "            for j in range(n_y):\n",
    "                if symmetric and j < i:\n",
    "                    continue\n",
    "                # Get the pairwise similarity matrix for x_i and y_j\n",
    "                C_ij = self.get_pairwise_matrix(x[i], y[j])\n",
    "                \n",
    "                # Calculate the global similarity (mean of pairwise similarities)\n",
    "                k_ij = self.get_global_similarity(C_ij)\n",
    "                \n",
    "                # Store the global similarity in the kernel matrix\n",
    "                K_ij[i, j] = k_ij # (1, 1)\n",
    "                \n",
    "                # If symmetric, copy the value to the (j, i) position as well\n",
    "                if symmetric and j != i:\n",
    "                    K_ij[j, i] = k_ij\n",
    "\n",
    "        # Normalize the kernel matrix if needed\n",
    "        if self.normalize_kernel:\n",
    "            if symmetric:\n",
    "                k_ii = torch.diagonal(K_ij)\n",
    "                x_k_ii_sqrt = torch.sqrt(k_ii)\n",
    "                y_k_ii_sqrt = x_k_ii_sqrt\n",
    "            else:\n",
    "                # Calculate self-similarity for X\n",
    "                x_k_ii = torch.empty(n_x, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                for i in range(n_x):\n",
    "                    C_ii = self.get_pairwise_matrix(x[i])\n",
    "                    x_k_ii[i] = self.get_global_similarity(C_ii)\n",
    "\n",
    "                x_k_ii_sqrt = torch.sqrt(x_k_ii)\n",
    "                print('x_k_ii_sqrt:', x_k_ii_sqrt)\n",
    "                print('x_k_ii_sqrt shape:', x_k_ii_sqrt.shape)\n",
    "\n",
    "                # Calculate self-similarity for Y\n",
    "                y_k_ii = torch.empty(n_y, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                for i in range(n_y):\n",
    "                    C_ii = self.get_pairwise_matrix(y[i])\n",
    "                    y_k_ii[i] = self.get_global_similarity(C_ii)\n",
    "\n",
    "                y_k_ii_sqrt = torch.sqrt(y_k_ii) \n",
    "                print('y_k_ii_sqrt:', y_k_ii_sqrt)\n",
    "                print('y_k_ii_sqrt shape:', y_k_ii_sqrt.shape)\n",
    "\n",
    "            print('old K_ij shape:', K_ij.shape)\n",
    "            print('old K_ij:', K_ij)\n",
    "            K_ij /= torch.outer(x_k_ii_sqrt, y_k_ii_sqrt)\n",
    "            print('normalized K_ij shape:', K_ij.shape)\n",
    "            print('normalized K_ij:', K_ij)\n",
    "            \n",
    "        return K_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AverageKernelMultiDevice:\n",
    "    def __init__(self, metric, gamma, gpu_id=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metric (str): The pairwise metric used for calculating the local similarity, only \"laplacian\" is supported now.\n",
    "            gamma (float): Gamma parameter for Laplacian kernel. Use sklearn's default gamma.\n",
    "            gpu_id (int): The GPU device ID to be used for computation.\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "        self.gamma = gamma\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "    def get_pairwise_matrix(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Computes the pairwise similarity of atomic environments using Laplacian kernel on GPU.\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Feature vector for atoms in multiple structures (n_x, n_atoms_x, n_features).\n",
    "            Y (torch.Tensor): Feature vector for atoms in multiple structures (n_y, n_atoms_y, n_features).\n",
    "                              If None, the pairwise similarity is computed between the same structures in X.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Tensor (n_x, n_y, n_atoms_x, n_atoms_y) representing the pairwise similarities between X and Y.\n",
    "                          If Y is None, the returned matrix is of shape (n_x, n_atoms_x, n_atoms_x).\n",
    "        \"\"\"\n",
    "        # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        device = torch.device(f'cuda:{self.gpu_id}' if (torch.cuda.is_available() and self.gpu_id is not None) else 'cpu')\n",
    "        X = X.to(dtype=torch.float32, device=device)  # Shape: (n_x, n_atoms_x, n_features)\n",
    "\n",
    "        if self.metric == \"laplacian\":\n",
    "\n",
    "            # Normalization\n",
    "            if Y is None:\n",
    "                Y = X  # Shape: (n_x, n_atoms_x, n_features)\n",
    "                diff = torch.abs(X.unsqueeze(2) - Y.unsqueeze(1))  # Shape: (n_x, n_atoms_x, n_atoms_x, n_features)\n",
    "                dist = torch.sum(diff, dim=-1)  # Shape: (n_x, n_atoms_x, n_atoms_x)\n",
    "                K_ij = torch.exp(-self.gamma * dist) # Shape: (n_x, n_atoms_x, n_atoms_x)\n",
    "\n",
    "            else:\n",
    "                Y = Y.to(dtype=torch.float32, device=device) # Shape: (n_y, n_atoms_y, n_features)\n",
    "\n",
    "                # Broadcast difference calculation: compute |X_i - Y_j| for all i, j pairs\n",
    "                diff = torch.abs(X.unsqueeze(1).unsqueeze(3) - Y.unsqueeze(0).unsqueeze(2))  # Shape: (n_x, n_y, n_atoms_x, n_atoms_y, n_features)\n",
    "\n",
    "                # Sum over the atoms dimension (dim=3 for X and dim=4 for Y)\n",
    "                dist = torch.sum(diff, dim=-1)  # Shape: (n_x, n_y, n_atoms_x, n_atoms_y)\n",
    "\n",
    "                # Sum over atoms (2nd and 3rd dims) to get the pairwise kernel value for each pair of molecules\n",
    "                K_ij = torch.exp(-self.gamma * dist)  # Shape: (n_x, n_y, n_atoms_x, n_atoms_y)\n",
    "        \n",
    "        print('X shape:', X.shape, 'Y shape:', Y.shape)\n",
    "        print('diff shape:', diff.shape)\n",
    "        print('dist shape:', dist.shape)\n",
    "        print('K_ij shape:', K_ij.shape)\n",
    "\n",
    "        return K_ij\n",
    "\n",
    "    def get_global_similarity(self, localkernel):\n",
    "        \"\"\"\n",
    "        Computes the average global similarity between two structures.\n",
    "        \n",
    "        Args:\n",
    "            localkernel (torch.Tensor): Tensor (n_x, n_y, n_atoms_x, n_atoms_y) representing the pairwise similarities between structures in X and Y.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Tensor (n_x, n_y) representing the average similarity between the structures.\n",
    "                          If normalization mode in get_pairwise_matrix(), the shape returned is tensor (n_x).\n",
    "        \"\"\"\n",
    "        device = torch.device(f'cuda:{self.gpu_id}' if (torch.cuda.is_available() and self.gpu_id is not None) else 'cpu')\n",
    "        localkernel = localkernel.clone().detach().to(dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Average similarity across all atoms in both molecules\n",
    "        K_ij = torch.mean(localkernel, dim=(-2, -1))  # Shape: (n_x, n_y) or (n_x)\n",
    "        # print('K_ij shape:', K_ij.shape)\n",
    "        return K_ij\n",
    "\n",
    "    def create(self, x, y=None):\n",
    "        \"\"\"\n",
    "        Creates the kernel matrix based on the given lists of local features x and y.\n",
    "    \n",
    "        Args:\n",
    "            x (iterable): A list of local feature arrays for each structure. Each element is a tensor of shape (n_atoms, n_features).\n",
    "            y (iterable): An optional second list of features. \n",
    "                          If not specified, y is assumed to be the same as x, and the function computes self-similarity.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor representing the pairwise global similarity kernel K[i,j] between the given structures. \n",
    "                          Shape: (n_x, n_y) or (n_x), depending on whether y is provided.\n",
    "        \"\"\"\n",
    "\n",
    "        # If y is None, compute self-similarity using x only\n",
    "        if y is None:\n",
    "            x_tensor = torch.stack([torch.tensor(i, dtype=torch.float32) for i in x])\n",
    "            localkernel = self.get_pairwise_matrix(x_tensor)\n",
    "            K_ij = self.get_global_similarity(localkernel)\n",
    "            K_ij = torch.sqrt(K_ij)\n",
    "\n",
    "            print('y=None')\n",
    "\n",
    "        # If y is provided, compute pairwise similarity between x and y\n",
    "        else:\n",
    "\n",
    "            # symmetric = False\n",
    "            # if y is None:\n",
    "            #     y = x\n",
    "                # symmetric = True\n",
    "            \n",
    "            # Convert input features to tensors\n",
    "            x_tensor = torch.stack([torch.tensor(i, dtype=torch.float32) for i in x])\n",
    "            y_tensor = torch.stack([torch.tensor(i, dtype=torch.float32) for i in y])\n",
    "\n",
    "            # Compute pairwise kernel between structures in x and y\n",
    "            localkernel = self.get_pairwise_matrix(x_tensor, y_tensor)\n",
    "\n",
    "            # Compute global similarity between structures in x and y\n",
    "            K_ij = self.get_global_similarity(localkernel)\n",
    "\n",
    "        # Normalize kernel if required\n",
    "        # if self.normalize_kernel:\n",
    "        #     if symmetric:\n",
    "        #         print(11111)\n",
    "        #         k_ii = torch.diagonal(K_ij)\n",
    "        #         x_k_ii_sqrt = torch.sqrt(k_ii).view(-1)\n",
    "        #         y_k_ii_sqrt = x_k_ii_sqrt\n",
    "        #     else:\n",
    "        #         # Calculate self-similarity for X\n",
    "        #         C_ii = self.get_pairwise_matrix(x_tensor)\n",
    "        #         x_k_ii = self.get_global_similarity(C_ii)\n",
    "        #         print('x_k_ii', x_k_ii.shape)\n",
    "        #         x_k_ii_sqrt = torch.sqrt(x_k_ii)\n",
    "        #         print('x_k_ii_sqrt', x_k_ii_sqrt.shape)\n",
    "\n",
    "        #         # Calculate self-similarity for Y\n",
    "        #         C_ii = self.get_pairwise_matrix(y_tensor)\n",
    "        #         y_k_ii = self.get_global_similarity(C_ii)\n",
    "        #         print('y_k_ii', y_k_ii.shape)\n",
    "        #         y_k_ii_sqrt = torch.sqrt(y_k_ii)\n",
    "        #         print('y_k_ii_sqrt', y_k_ii_sqrt.shape)\n",
    "\n",
    "        print('old K_ij shape:', K_ij.shape)\n",
    "        print('old K_ij:', K_ij)\n",
    "            # K_ij /= torch.outer(x_k_ii_sqrt, y_k_ii_sqrt)\n",
    "            # print('normalized K_ij shape:', K_ij.shape)\n",
    "            # print('normalized K_ij:', K_ij)\n",
    "\n",
    "        return K_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_pytorch(cand_soap, ref_soap=None, kernel_metric=\"laplacian\", gpu_id=None):\n",
    "    \"\"\"\n",
    "    Computes the pairwise similarity between candidate and reference SOAP descriptors using laplacian kernel metric.\n",
    "    \"\"\"\n",
    "    # 以 sci-kit learn 相同的方法计算 gamma 值\n",
    "    gamma = 1.0 / cand_soap[0].shape[1]\n",
    "\n",
    "    re = AverageKernelMultiDevice(metric=kernel_metric, gamma=gamma, gpu_id=gpu_id)\n",
    "    return re.create(cand_soap, ref_soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_update_structures(ref_structures, cand_structures, njobs=4, gpu=1, batch_size=50, species=[\"H\", \"C\", \"O\", \"N\"], r_cut=10.0, n_max=6, l_max=4, threshold=0.9, logger=None):\n",
    "    round_num = 0\n",
    "    logger.info(f\"njobs: {njobs}, gpu: {gpu}, batch_size: {batch_size}\")\n",
    "    logger.info(f\"species: {species}, r_cut: {r_cut}, n_max: {n_max}, l_max: {l_max}, threshold: {threshold}\")\n",
    "\n",
    "    while True:\n",
    "        round_num += 1\n",
    "\n",
    "        if round_num == 1:\n",
    "\n",
    "            # 初次计算，先计算全部的 SOAP 描述符\n",
    "            soap_ref = compute_soap_descriptors(ref_structures, njobs, species, r_cut, n_max, l_max, logger)\n",
    "            soap_cand = compute_soap_descriptors(cand_structures, njobs, species, r_cut, n_max, l_max, logger)\n",
    "\n",
    "            # 如果 soap_ref 为空，则将 soap_cand 的第一个元素添加到 soap_ref 中\n",
    "            if soap_ref == []:\n",
    "                ref_structures.append(cand_structures[0])\n",
    "                soap_ref.append(soap_cand[0])\n",
    "                logger.info(\"Ref structure is empty, add the first Cand structure to Ref structure\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # 支持 GPU 计算\n",
    "            # 目前仅支持单 GPU 计算\n",
    "            if gpu:\n",
    "                # 分 batch_size 批次计算 cand 中每个结构与 ref 中所有结构的相似度\n",
    "                re_kernel_results = Parallel(n_jobs=gpu)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_cand[i:i+batch_size], soap_ref, gpu_id=(i//batch_size)%gpu) \n",
    "                    for i in range(0, len(soap_cand), batch_size))\n",
    "\n",
    "                # 分别计算 cand 与 ref 中结构的自我相似度，用于正则化最终的相似度结果到 [0, 1]\n",
    "                soap_cand_self = Parallel(n_jobs=gpu)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_cand[i:i+batch_size], gpu_id=(i//batch_size)%gpu) \n",
    "                    for i in range(0, len(soap_cand), batch_size))\n",
    "\n",
    "                soap_ref_self = Parallel(n_jobs=gpu)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_ref[i:i+batch_size], gpu_id=(i//batch_size)%gpu) \n",
    "                    for i in range(0, len(soap_ref), batch_size))\n",
    "\n",
    "                # 全部数据一起计算会内存不足\n",
    "                # re_kernel = compute_similarity_pytorch(soap_cand, soap_ref)\n",
    "                # soap_cand_self = compute_similarity_pytorch(soap_cand)\n",
    "                # soap_ref_self = compute_similarity_pytorch(soap_ref)\n",
    "                \n",
    "                # soap_cand_self = Parallel(n_jobs=njobs)(delayed(compute_similarity_pytorch)(soap_cand[i:i+cand_batch_size]) for i in range(0, len(soap_cand), cand_batch_size))\n",
    "                # soap_ref_self = Parallel(n_jobs=njobs)(delayed(compute_similarity_pytorch)(soap_ref[i:i+cand_batch_size]) for i in range(0, len(soap_ref), cand_batch_size))\n",
    "                \n",
    "                # 此处正则化需要根据 batch_size 修改\n",
    "                # re_kernel /= torch.outer(soap_cand_self, soap_ref_self)\n",
    "                # re_kernel = re_kernel.cpu()\n",
    "\n",
    "                # 依旧在 GPU 上计算\n",
    "                # nor\n",
    "                # re_kernel_normalized_results\n",
    "\n",
    "                # 非并行便于调试\n",
    "                # re_kernel_results = []\n",
    "                # for i in range(len(soap_cand)):\n",
    "                #     # print(i)\n",
    "                #     result = compute_similarity_pytorch(soap_cand[i:i+1], soap_ref)\n",
    "                #     re_kernel_results.append(result)\n",
    "                    \n",
    "                # re_kernel = np.vstack(re_kernel_results.cpu())\n",
    "\n",
    "            # 支持多核 CPU 并行运算\n",
    "            else:\n",
    "                re_kernel_results = Parallel(n_jobs=njobs)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_cand[i:i+1], soap_ref) \n",
    "                    for i in range(len(soap_cand)))\n",
    "\n",
    "                soap_cand_self = Parallel(n_jobs=njobs)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_cand[i:i+1]) \n",
    "                    for i in range(len(soap_cand)))\n",
    "\n",
    "                soap_ref_self = Parallel(n_jobs=njobs)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_ref[i:i+1]) \n",
    "                    for i in range(len(soap_ref)))\n",
    "\n",
    "            # re_kernel = np.vstack(re_kernel_results.cpu().numpy())\n",
    "            # soap_cand_self = np.vstack(soap_cand_self.cpu().numpy())\n",
    "            # soap_ref_self = np.vstack(soap_ref_self.cpu().numpy())\n",
    "            # re_kernel /= np.outer(soap_cand_self, soap_ref_self)\n",
    "                                      \n",
    "            # 合并批次/并行计算的结果\n",
    "            re_kernel = torch.cat(re_kernel_results, dim=0)\n",
    "            soap_cand_self = torch.cat(soap_cand_self, dim=0)\n",
    "            soap_ref_self = torch.cat(soap_ref_self, dim=0)\n",
    "\n",
    "            # 正则化相似度矩阵\n",
    "            re_kernel /= torch.outer(soap_cand_self, soap_ref_self)\n",
    "\n",
    "            # 将相似度矩阵移动到 CPU 以避免 I/O 造成的计算速度瓶颈\n",
    "            re_kernel = re_kernel.cpu()\n",
    "\n",
    "            end_time = time.time()\n",
    "            logger.info(f\"Round {round_num}: Similarity computation and self-similarity completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            print('Round:', round_num)\n",
    "            print('re_kernel shape:', re_kernel.shape)\n",
    "            print('re_kernel:', re_kernel[:10])\n",
    "\n",
    "            # 选取 cand_structures 中每个结构与 ref_structures 中所有结构的最大相似度\n",
    "            max_similarity_values, _ = torch.max(re_kernel, dim=1)\n",
    "\n",
    "            # max_similarity_values = np.max(re_kernel, axis=1)\n",
    "            # print(max_similarity_values.shape)\n",
    "\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "\n",
    "            if gpu:\n",
    "                re_kernel_results = Parallel(n_jobs=gpu)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_cand[i:i+batch_size], [soap_ref[-1]], gpu_id=(i//batch_size)%gpu) \n",
    "                    for i in range(0, len(soap_cand), batch_size))\n",
    "            else:\n",
    "                re_kernel_results = Parallel(n_jobs=njobs)(\n",
    "                    delayed(compute_similarity_pytorch)(soap_cand[i:i+1], [soap_ref[-1]]) \n",
    "                    for i in range(len(soap_cand)))\n",
    "                                \n",
    "            re_kernel = torch.cat(re_kernel_results, dim=0)\n",
    "            # re_kernel = np.vstack(re_kernel_results.cpu().numpy())\n",
    "\n",
    "            # 内存不足\n",
    "            # re_kernel = compute_similarity_pytorch(soap_cand, soap_ref)\n",
    "            # print('sxmi')\n",
    "            # print(soap_cand_self.shape, soap_ref_self.shape)\n",
    "\n",
    "            # 此处正则化需要根据 batch_size 修改\n",
    "            re_kernel /= torch.outer(soap_cand_self, soap_ref_self[-1].unsqueeze(0))\n",
    "            # re_kernel /= torch.outer(soap_cand_self, soap_ref_self)\n",
    "            # re_kernel = re_kernel.cpu()\n",
    "            # re_kernel /= np.outer(soap_cand_self, soap_ref_self)\n",
    "\n",
    "            # re_kernel = np.vstack(re_kernel_results.cpu())\n",
    "            end_time = time.time()\n",
    "            logger.info(f\"Round {round_num}: Similarity computation completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            print('Round:', round_num)\n",
    "            print('re_kernel shape:', re_kernel.shape)\n",
    "            print('re_kernel:', re_kernel[:10])\n",
    "\n",
    "            # max_similarity_values = np.max(np.column_stack((max_similarity_values, np.max(re_kernel, axis=1))), axis=1)\n",
    "            # print(max_similarity_values.shape)\n",
    "            # print(torch.max(re_kernel, dim=1)[0].view(-1, 1).shape)\n",
    "            # print(max_similarity_values.view(-1, 1).shape)\n",
    "            # print(torch.cat((max_similarity_values.view(-1, 1), torch.max(re_kernel, dim=1)[0].view(-1, 1)), dim=1).shape)\n",
    "            \n",
    "            # 将原先 max_similarity_values 与新加入的 ref 计算的 max_similarity_value 合并\n",
    "            max_similarity_values, _ = torch.max(\n",
    "                                            torch.cat((max_similarity_values.view(-1, 1), \n",
    "                                                       torch.max(re_kernel, dim=1)[0].view(-1, 1)), \n",
    "                                                       dim=1), \n",
    "                                            dim=1)\n",
    "            \n",
    "        # 删除 cand_structures 中与 ref_structures 中相似度高于 threshold 的所有结构\n",
    "        # 更新 soap_cand, cand_structures, max_similarity_values, soap_cand_self\n",
    "        old_cand_num = len(cand_structures)\n",
    "\n",
    "        preserve_condition = max_similarity_values < threshold\n",
    "        soap_cand = list(compress(soap_cand, preserve_condition)) # itertools.compress() 更高效\n",
    "        cand_structures = list(compress(cand_structures, preserve_condition))\n",
    "        max_similarity_values = max_similarity_values[preserve_condition] # 布尔索引更高效\n",
    "        soap_cand_self = soap_cand_self[preserve_condition]\n",
    "\n",
    "        new_cand_num = len(cand_structures)\n",
    "        logger.info(f\"Round {round_num}: Cand structures reduced from {old_cand_num} to {new_cand_num}\")\n",
    "\n",
    "        # 如果 cand_structures 中没有元素，说明筛选完毕，退出循环\n",
    "        if new_cand_num == 0:\n",
    "            break\n",
    "        \n",
    "        # 将 cand_structures 与 ref_structures 中最不相似的结构添加到 ref_structures 中\n",
    "        # 包含结构，SOAP 描述符，以及用于正则化的自我相似度\n",
    "\n",
    "        # min_max_similarity = np.min(max_similarity_values).round(5)\n",
    "        min_max_similarity = torch.min(max_similarity_values).item()\n",
    "\n",
    "        # min_max_similarity_index = np.argmin(max_similarity_values)\n",
    "        min_max_similarity_index = torch.argmin(max_similarity_values).item()\n",
    "        \n",
    "        ref_structures.append(cand_structures[min_max_similarity_index])\n",
    "        soap_ref.append(soap_cand[min_max_similarity_index])\n",
    "\n",
    "        # print(soap_ref_self)\n",
    "        # print(soap_cand_self[min_max_similarity_index].unsqueeze(0))\n",
    "        soap_ref_self = torch.cat((soap_ref_self, soap_cand_self[min_max_similarity_index].unsqueeze(0)))\n",
    "        # soap_ref_self = np.vstak((soap_ref_self, soap_cand_self[min_max_similarity_index]))\n",
    "        \n",
    "\n",
    "        logger.info(f\"Round {round_num}: Added structure with min max similarity {min_max_similarity:.5f}.\")\n",
    "        logger.info(f\"Ref structures: {len(ref_structures)}, Cand structures: {len(cand_structures)}\")\n",
    "        logger.info(\"---------\")\n",
    "\n",
    "    logger.info(\"No structures remaining in candidate list.\")\n",
    "    logger.info(f\"Ref structures: {len(ref_structures)}, Cand structures: {len(cand_structures)}\")\n",
    "    logger.info(\"---------\")\n",
    "\n",
    "    return ref_structures, soap_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主程序\n",
    "def main(ref_file, cand_file, njobs, gpu, batch_size, r_cut, n_max, l_max, threshold):\n",
    "    total_logger = setup_total_logging()\n",
    "    total_logger.info('Total Log begin')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 读取数据\n",
    "    if ref_file == '':\n",
    "        ref_structures = []\n",
    "    else:\n",
    "        ref_structures = read(ref_file, index=':')\n",
    "    cand_structures = read(cand_file, index=':')\n",
    "\n",
    "    # 根据 chemical_formula 分组\n",
    "    ref_dict = defaultdict(list)\n",
    "    cand_dict = defaultdict(list)\n",
    "\n",
    "    for structure in ref_structures:\n",
    "        formula = structure.get_chemical_formula()\n",
    "        ref_dict[formula].append(structure)\n",
    "\n",
    "    for structure in cand_structures:\n",
    "        formula = structure.get_chemical_formula()\n",
    "        cand_dict[formula].append(structure)\n",
    "\n",
    "    formula_num = len(cand_dict.keys())\n",
    "    total_logger.info(f\"There are {formula_num} formulas to process.\")\n",
    "\n",
    "    # 确认 species 中所含有的元素类型\n",
    "    species = set()\n",
    "    for key in cand_dict:\n",
    "        species.update(cand_dict[key][0].get_chemical_symbols())\n",
    "    for key in ref_dict:\n",
    "        try:\n",
    "            species.update(cand_dict[key][0].get_chemical_symbols())\n",
    "        except:\n",
    "            continue\n",
    "    species = list(species)\n",
    "    total_logger.info(f\"Species: {species}\")\n",
    "    total_logger.info(\"---------\")\n",
    "\n",
    "    for i, formula in enumerate(cand_dict.keys()):\n",
    "        # 如果 ref_dict 中没有该组，则会返回空列表，程序可以正常运行\n",
    "        total_logger.info(f\"Processing formula {i+1:>}/{formula_num:>}: {formula}\")\n",
    "        total_logger.info(f\"Start Ref structures: {len(ref_dict[formula])}, Cand structures: {len(cand_dict[formula])}\")\n",
    "\n",
    "        logger = setup_logging(formula)  \n",
    "        formula_start_time = time.time()\n",
    "        logger.info('Log begin')\n",
    "        logger.info(f\"Processing formula: {formula}\")\n",
    "\n",
    "        # 由于后续相似度是经过正则化到 [0, 1] 的，因此不必对每一个反应自动匹配 species，直接取所有元素的并集即可\n",
    "        # species=list(set(cand_dict[formula][0].get_chemical_symbols()))\n",
    "        updated_structures, updated_soap_list = compare_and_update_structures(ref_dict[formula], \n",
    "                                                                              cand_dict[formula], \n",
    "                                                                              njobs=njobs,\n",
    "                                                                              gpu=gpu,\n",
    "                                                                              batch_size=batch_size,                                                                                  \n",
    "                                                                              species=species,\n",
    "                                                                              r_cut=r_cut,\n",
    "                                                                              n_max=n_max,\n",
    "                                                                              l_max=l_max,\n",
    "                                                                              threshold=threshold,\n",
    "                                                                              logger=logger)\n",
    "        \n",
    "        # 逐一保存更新后的参考结构\n",
    "        write(os.path.join(formula, f\"updated_ref_structures_{formula}.xyz\"), updated_structures)\n",
    "        logger.info(f\"Updated reference structures saved to '{formula}/updated_ref_structures_{formula}.xyz'\")\n",
    "        \n",
    "        # 保存更新后的结构和 SOAP 到 HDF5\n",
    "        soap_dict = defaultdict(list)\n",
    "        for i in range(len(updated_soap_list)):\n",
    "            soap_result = updated_soap_list[i]\n",
    "            soap_dict[formula].append(soap_result)\n",
    "        save_soap_to_hdf5(soap_dict, os.path.join(formula, f\"updated_ref_soap_descriptors_{formula}.h5\"))\n",
    "        logger.info(f\"SOAP descriptors saved to '{formula}/updated_ref_soap_descriptors_{formula}.h5'\")\n",
    "\n",
    "        formula_end_time = time.time()\n",
    "        logger.info(f\"Done! Total time elapsed: {formula_end_time - formula_start_time:.2f} seconds\")\n",
    "        logger.info('Log end')\n",
    "\n",
    "        total_logger.info(f\"End Ref structures: {len(updated_structures)}\")\n",
    "        total_logger.info(f\"Done! Total time elapsed: {formula_end_time - formula_start_time:.2f} seconds\")\n",
    "        total_logger.info(\"---------\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_logger.info('All reactions processed successfully!')\n",
    "    total_logger.info(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    total_logger.info('Total Log end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1\n",
      "re_kernel shape: torch.Size([100, 1])\n",
      "re_kernel: tensor([[1.0000],\n",
      "        [0.9604],\n",
      "        [0.9210],\n",
      "        [0.8936],\n",
      "        [0.8580],\n",
      "        [0.8394],\n",
      "        [0.8287],\n",
      "        [0.8183],\n",
      "        [0.8113],\n",
      "        [0.8064]])\n",
      "Round: 2\n",
      "re_kernel shape: torch.Size([86, 1])\n",
      "re_kernel: tensor([[0.8458],\n",
      "        [0.8745],\n",
      "        [0.8965],\n",
      "        [0.9338],\n",
      "        [0.9460],\n",
      "        [0.9602],\n",
      "        [0.9817],\n",
      "        [1.0000],\n",
      "        [0.8437],\n",
      "        [0.9001]])\n",
      "Round: 3\n",
      "re_kernel shape: torch.Size([73, 1])\n",
      "re_kernel: tensor([[0.9103],\n",
      "        [0.9322],\n",
      "        [0.9301],\n",
      "        [0.9105],\n",
      "        [0.8951],\n",
      "        [0.9616],\n",
      "        [0.9552],\n",
      "        [0.9723],\n",
      "        [0.9167],\n",
      "        [0.9296]])\n",
      "Round: 4\n",
      "re_kernel shape: torch.Size([63, 1])\n",
      "re_kernel: tensor([[0.9624],\n",
      "        [0.9588],\n",
      "        [0.9145],\n",
      "        [0.8894],\n",
      "        [0.8773],\n",
      "        [0.9138],\n",
      "        [0.9587],\n",
      "        [0.9606],\n",
      "        [1.0000],\n",
      "        [0.9220]])\n",
      "Round: 5\n",
      "re_kernel shape: torch.Size([41, 1])\n",
      "re_kernel: tensor([[0.9420],\n",
      "        [0.9757],\n",
      "        [0.9579],\n",
      "        [0.8493],\n",
      "        [0.9574],\n",
      "        [0.9171],\n",
      "        [1.0000],\n",
      "        [0.9741],\n",
      "        [0.9508],\n",
      "        [0.9130]])\n",
      "Round: 6\n",
      "re_kernel shape: torch.Size([23, 1])\n",
      "re_kernel: tensor([[0.9827],\n",
      "        [0.8867],\n",
      "        [0.9666],\n",
      "        [0.9623],\n",
      "        [0.9690],\n",
      "        [0.9621],\n",
      "        [1.0000],\n",
      "        [0.9733],\n",
      "        [0.8912],\n",
      "        [0.9021]])\n",
      "Round: 7\n",
      "re_kernel shape: torch.Size([6, 1])\n",
      "re_kernel: tensor([[1.0000],\n",
      "        [0.9826],\n",
      "        [0.8398],\n",
      "        [0.9523],\n",
      "        [0.9899],\n",
      "        [0.8378]])\n",
      "Round: 8\n",
      "re_kernel shape: torch.Size([2, 1])\n",
      "re_kernel: tensor([[0.9937],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "ref = ''\n",
    "cand = 'rxn0000.xyz'\n",
    "njobs = 8\n",
    "gpu = 0\n",
    "batch_size = 50\n",
    "r_cut = 10.0\n",
    "n_max = 6\n",
    "l_max = 4\n",
    "threshold = 0.95\n",
    "main(ref, cand, njobs, gpu, batch_size, r_cut, n_max, l_max, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([2, 9, 1500])\n",
    "y = torch.rand([100, 9, 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.unsqueeze(0).unsqueeze(2) \n",
    "y = y.unsqueeze(1).unsqueeze(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 1, 9, 1500]), torch.Size([100, 1, 9, 1, 1500]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2, 9, 9, 1500])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x-y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5924]) tensor([0.3228])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5924, 0.3228])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand([1, ])\n",
    "b = torch.rand([1, ])\n",
    "print(a, b)\n",
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "555%1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
